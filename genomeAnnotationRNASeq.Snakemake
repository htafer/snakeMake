#run snaxli kemake
#
#snakemake -d `pwd` -s `pwd`/genomeAnnotationRNASeq.Snakemake --stats snakemake.stats -j 100 --cluster 'sbatch {params.cluster}'

#################################
#                               #
#     Import modules            #
#                               #
#################################

import math
import os

#################################
#                               #
#      Own function definition  #
#                               #
#################################



def getFileNumber(file_name, entryNumber):
      f = open(file_name)
      total = 0
      for line in f:
          if ">" in line:
              total += 1
      f.close()
      return math.floor(2*total/entryNumber+1);    


#get bam file
def getBamFile(dir,species):
      list=[];
      for file in os.listdir(dir+'/'+species+'.reads/.'):
            if fnmatch.fnmatch(file,'*.*.bam'):
                  list.append(file);
      return list

#uniquify ids
def f10(seq, idfun=None):
    seen = set()
    results=[]
    if idfun is None:
        for x in seq:
            if x in seen:
                continue
            seen.add(x)
            results.append(x)
    else:
        for x in seq:
            x = idfun(x)
            if x in seen:
                continue
            seen.add(x)
            results.append(x)
    return results



#################################
#                               #
#      Variables setup          #
#                               #
#################################

    #################################                
    #Environment                    #
    #################################
WORKDIR="/home/lv70539/htafer/genomeAnnotationTranscript"
#WORKDIR="/media/vsc2/genomeAnnotation"
#WORKDIR="/home/htafer/annotation/"
#WORKDIR="/media/work/genomeAnnotation"
#WORKDIR="/home/htafer/genomeAnnotationNew"
#COMPUTEDIR="/tmp"

COMPUTEDIR="/global/lv70539/htafer"
#COMPUTEDIR="/scratch"

    #################################
    #THREADS                        #
    #################################

THREADS=16

    ##################################
    #FILES                           #
    ##################################

#GENOME
ID="claImm";
REF=ID+".fasta"

#BAMFILES
BAMFILES="./reads/{samples}.fastq"
BAMS ,= glob_wildcards(BAMFILES)
print(BAMS);

#EVMWEIGHT
EVMWEIGHT=WORKDIR+"/weightFile.cfg"

#DN from genomeAnnotationDN
#DN=WORKDIR+"/cImmundaBroad.gff"
DNGFF=WORKDIR+"/claImm.allEvm2.gff"
DNGTF=WORKDIR+"/claImm.allEvm2.gtf"

PFAM="/global/lv70539/htafer/share/"
BLAST="/global/lv70539/htafer/share/eurotiomycetesUniref90.clean.fasta"

    ##################################
    #BIOLOGY                         #
    ##################################

#BIOLOGY
INTRON=2000


#Which rules are run locally # depends on which computer the pipeline is running
#localrules Segemehl
#localrules: all, clean, segemehlIdx, bamToFastq, composeMerge, mergeAssemblies, PASAtrainingset, PASAhints, prepareGFFforEVM, mergeFastq

#Localrules STAR
localrules: all, clean, composeMerge, mergeAssemblies, PASAtrainingset, PASAhints, prepareGFFforEVM, mergeFastq, STARIdx,otherTranscript, lncRNA

#cluster: STAR, mergeBam,  cufflinks, trinityDeNovo, trinityAlignment, normalization, PASA, augustus, EVM, postPASA

#localrules: all, clean, STAR, STARIdx, bamToFastq, mergeBam, cufflinks, composeMerge, mergeAssemblies, normalization, trinityAlignment, trinityDeNovo, PASA, PASAtrainingset, PASAhints, augustus, mergeFastq, PASAtrainingset, prepareGFFforEVM, EVM, postPASA

##################################
##                               #
##      ALL                      #
##                               #
##################################


rule all:
     input: ID+".noKnownFunc.gff3"


##################################################################################                              
#                                                                                #
#                  Get The Rest                                                  #
#                                                                                #
##################################################################################                              

rule otherTranscript:
     input: lnc=ID+".lncRNA.gff3", coding="./gff/"+ID+".allProtein.gff3", pasaAssemblies="./reads/"+ID+".PASA/"+ID+".pasa_assemblies.gff3"
     output: ID+".noKnownFunc.gff3"
     shell:"""    
     mergealign.pl < {input.pasaAssemblies} | cut -f 1-9 >  merged.gff 
     bedtools intersect -s -v -a merged.gff -b {input.lnc} | bedtools intersect -s -v -a - -b {input.coding} | cut -f 9  | cut -f 1 -d " " | sort -u | fgrep -wf - reads/claImm.PASA/claImm.pasa_assemblies.gff3 > {output}
     """


#################################################################################                              
#                                                                               #
#                  Get lncRNAs                                                  #
#                                                                               #
#################################################################################                              
########################            
####    lncRNA      ####            
########################            

rule lncRNA:
     input: ids="./CPAT/cpat.results", blast="BLAST/blast.coding.candidates", pfam="PFAM/"+ID+".pfam.coding.candidates", pasaAssemblies="./reads/"+ID+".PASA/"+ID+".pasa_assemblies.gff3"
     output: lnc=ID+".lncRNA.gff3"
     threads: THREADS
     shell:"""                      
     cat {WORKDIR}/{input.blast} {WORKDIR}/{input.pfam} | sort -u > coding.candidates                               
     cut -f 1 {WORKDIR}/{input.ids}  > id.cpat.results                      
     cat id.cpat.results | perl -lane 'my $count=`grep -cw $F[0] coding.candidates`; chomp($count); if($count==0){{print $F[0]}};' | sort -u  | fgrep -w -f - {WORKDIR}/{input.pasaAssemblies} > {WORKDIR}/{output.lnc}                     
     """
########################            
####      BLAST     ####            
########################            

rule blast:
     input: cpat="./CPAT/"+ID+".cpat.fasta"
     output: blast="BLAST/blast.coding.candidates"
     threads: THREADS
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     shell:"""
     mkdir -p BLAST
     cd BLAST                       
     #formatdb -i {BLAST} -p T -n uniprot90S                                 
     blastx -query {WORKDIR}/{input.cpat} -db {BLAST} -num_threads {threads} -outfmt 7 -out blast.out            
     cat blast.out | perl -lane 'if($F[7]>$F[6] && $F[10]<0.001){{print;}}' | cut -f 1 | sort -u | sed -r 's/asmbl/>asmbl/' > {WORKDIR}/{output.blast}      
     """

########################            
####     PFAM       ####            
########################            

rule pfam:
     input: "./CPAT/"+ID+".cpat.fasta"
     output: "PFAM/"+ID+".pfam.coding.candidates"
     threads: THREADS
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     shell: """                     
     mkdir -p PFAM                  
     cd PFAM                        
     pfam_scan.pl -fasta {WORKDIR}/{input} -dir {PFAM} -cpu {threads} -outfile pfam.out                             
     cat pfam.out | grep -v '#' | perl -lane 'if($F[2]>$F[1] && $F[12]<0.001){{print;}}' | cut -f 1 | sort -u | sed -r 's/asmbl/>asmbl/' > {WORKDIR}/{output}                                     
     """



################                    
#### CPAT   ####                    
################                    

rule cpat:
     input: gene="noOverlap.fasta", fa="assemblies.fa", nonc="nonconding.fasta", coding="codingseq.fasta"
     output: fasta="./CPAT/"+ID+".cpat.fasta", results="./CPAT/cpat.results"
     threads: THREADS
     shell:"""                      
     mkdir -p CPAT
     cd CPAT
     make_hexamer_tab.py -c {WORKDIR}/{input.coding} -n {WORKDIR}/{input.nonc} > hexamer.tab
     make_logitModel.py -c {WORKDIR}/{input.coding} -n {WORKDIR}/{input.nonc} -x hexamer.tab -o {ID}
     module load python/2.7
     cpat.py -d {ID}.logit.RData -x hexamer.tab -g {WORKDIR}/{input.gene} -o {ID}.cpat
     cat {ID}.cpat |  perl -lane 'if($F[5]<0.01){{print;}}' | sort -k 2,2gr | sed -r 's/ASMBL/asmbl/' > {WORKDIR}/{output.results}
     parallel -j {threads} 'grep -P "{{}}$" {WORKDIR}/{input.fa} -A 1 ' ::: `cat {WORKDIR}/{output.results}  | cut -f 1 | grep asmbl` > {WORKDIR}/{output.fasta}
     """


rule prepareFiles:
     input: fa="assemblies.fa", intron="./gff/"+ID+".PASAhints.gff", updateAnn="./gff/"+ID+".allProtein.gff3"
     output: coding="codingseq.fasta", nonc="nonconding.fasta"
     params:
     threads: THREADS
     shell:"""                      
     #We have to prepare a training data set in order to train CPAT         
     #Start with the protein dataset
     getAnnoFasta.pl {input.updateAnn} --seqfile {REF}                      
     cat "./gff/{ID}.allProtein3.cdsexons | sed -r 's/.cds[0-9]//g' | perl -lane 'BEGIN{{my $previousId="";}} if($F[0]=~/>/){{if($F[0] eq $previousId){{next;}}else{{$previousId=$F[0]; print "\n",$F[0]}}}}else{{printf $F[0]}}' > {output.coding}
     #Now generate the non coding dataset with the intron                   
     cat {input.intron} | sed -r 's/;src=E//g' > temp.introns               
     parallel -j {threads} 'grep -P "{{}}$" {input.fa} -A 1 ' ::: `cat temp.introns | cut -f 9 | sed -r 's/.+Target=([^$]+)/\\1/' | sort -u` > {output.nonc}
     """

################
#   INTERSECT  #
################
rule intersect:
     input: transAnn="./reads/"+ID+".PASAtrainingset/"+ID+".assemblies.fasta.transdecoder.genome.gff3", pasaAssemblies="./reads/"+ID+".PASA/"+ID+".pasa_assemblies.gff3", pasaSequence="./reads/"+ID+".PASA/"+ID+".assemblies.fasta", updateAnn="./gff/"+ID+".allProtein.gff3"
     output: fasta="noOverlap.fasta", fa="assemblies.fa"
     params:
     threads: THREADS
     shell:"""                      
     #put the gene together         
     mergealign.pl < {input.pasaAssemblies} | cut -f 1-9 >  merged.gff      
     #check which gene overlap neither with the updated protein annotation nor with the pasa protein annotation     
     intersectBed -v -s -a merged.gff -b {input.updateAnn} > no.gff3        
     intersectBed -v -s -a no.gff3 -b {input.transAnn} > noOverlap.gff      
     perl ~/bin/rmenterdb.pl < {input.pasaSequence} > {output.fa}           
     #Fetch the sequences           
     parallel -j {threads} 'grep -P "{{}}$" {output.fa} -A 1 ' ::: `cat noOverlap.gff | cut -f 9 | sed -r 's/.+Target=([^$]+)/\\1/' | sort -u` > {output.fasta} 
"""




#################################################################################
#                                                                               #
#                  Get New Proteins                                             #
#                                                                               #
#################################################################################                  

rule newProtein:
       input: transAnn="./reads/"+ID+".PASAtrainingset/"+ID+".assemblies.fasta.transdecoder.genome.gff3", updateAnn="./gff/"+ID+".Annotation/"+ID+".gff3"
       output: newProtein="./gff/"+ID+".newProtein.gff3", allProtein="./gff/"+ID+".allProtein.gff3"
       shell:"""       
       grep -P "\tCDS\t" {input.transAnn} | mergealign.pl > {ID}.transdecoder.CDS.gff3                 
       grep -P "\tCDS\t" {input.updateAnn} | mergealign.pl > {ID}.update.CDS.gff3                      
       bedtools intersect -v -a {ID}.transdecoder.CDS.gff3 -b {ID}.update.CDS.gff3 | cut -f 9 | sed -r 's/.+Parent=//' | fgrep -f - {input.transAnn} | grep -P "\tCDS\t" > {ID}.newProtein.CDS.gff3                            
       cut -f 9 {ID}.newProtein.CDS.gff3 | sed -r 's/.+Parent=//' | fgrep -f - {input.transAnn} | perl -lane 'if($F[2]=~/mRNA/){{$anno=$F[8]; $anno=~s/ID=.+Parent=/ID=/; $anno=~/=(asmbl[^g]+g[^;]+)/; my  $id=$1; $id=~s/g\./m./; $anno=~s/ORF/$id/; print "$F[0]\t$F[1]\tgene\t$F[3]\t$F[4]\t$F[5]\t$F[6]\t$F[7]\t$anno"; $anno=$F[8]; $anno=~s/ORF/$id/; print join("\t",@F[0..7]),"\t$ann\
o";}}else{{print;}}' > {output.newProtein}                     
       cat {input.updateAnn} {output.newProtein}  > {output.allProtein}                                
       """



#################################################################################
#                                                                               #
#                           PASA 2nd run                                        #
#                                                                               #
#################################################################################

rule postPASA:
      input: genome=ID+".fasta", pred="./gff/"+ID+".all.evm.gff3", dn="./reads/"+ID+".trinityDN", gg="./reads/"+ID+".trinityGG"
      output: dir="./gff/"+ID+".Annotation", updateGFF3=ID+".Annotation/"+ID+".gff3"
      params: cluster="--partition=mem_0064 --qos=normal_0064"
      threads: THREADS
      shell: """
      #mysql start
      cd $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/scripts/mysql_install_db --no-defaults --datadir=$HOME/bin/mysql-5.6.22/data
      cp $HOME/my-new.cnf $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/bin/mysqld_safe --defaults-file=./my-new.cnf --skip-grant-tables &
      sleep 10
      prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
      mkdir -p {COMPUTEDIR}/PASA${{prefix}}
      cd {COMPUTEDIR}/PASA${{prefix}}
      #check compatibility
      #$PASAHOME/misc_utilities/pasa_gff3_validator.pl {WORKDIR}/{input.pred}
      cat {WORKDIR}/{input.dn}/Trinity.fasta {WORKDIR}/{input.gg}/Trinity-GG.fasta > {COMPUTEDIR}/PASA${{prefix}}/transcripts.fasta
      #Prepare annotCompare.config
      DBNAME={ID}
      cat $PASAHOME/pasa_conf/pasa.annotationCompare.Template.txt | sed -r "s/DBNAME/${{DBNAME}}/" > ./annotCompare.config
      $PASAHOME/scripts/Launch_PASA_pipeline.pl -c ./annotCompare.config -A -L --annots_gff3 {WORKDIR}/{input.pred} -g {WORKDIR}/{input.genome} -t ./transcripts.fasta
      mv {COMPUTEDIR}/PASA${{prefix}} {WORKDIR}/{output.dir}
      cd {WORKDIR}/{output.dir} 
      mv *gene_structures_post_PASA_updates.*.gff3 {output.upateGFF3}
      #mysql shutdown
      cd $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/bin/mysqladmin --defaults-file=./my-new.cnf shutdown
      """




#################################################################################   
#                                                                               #     
#                            EVM                                                #     
#                                                                               #     
#################################################################################  

rule EVM:
     input: evmIn="./gff/"+ID+".evmIn.gff", pasa="./reads/"+ID+".PASA", genome=ID+".fasta"
     output: "./gff/"+ID+".all.evm.gff3"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     #partitioning the inputs
     $EVM_HOME/EvmUtils/partition_EVM_inputs.pl --genome {WORKDIR}/{input.genome} --gene_predictions {WORKDIR}/{input.evmIn} --transcript_alignments {WORKDIR}/{input.pasa}/{ID}.pasa_assemblies.gff3 --segmentSize 1000000 --overlapSize 20000 --partition_listing partitions_list.out
     #generating the EVM Command Set
     $EVM_HOME/EvmUtils/write_EVM_commands.pl --genome {WORKDIR}/{input.genome} --weights {EVMWEIGHT} --gene_predictions {WORKDIR}/{input.evmIn} --transcript_alignments {WORKDIR}/{input.pasa}/{ID}.pasa_assemblies.gff3 --output_file evm.out --partitions partitions_list.out  --min_intron_length 10 > commands.list
     #$EVM_HOME/EvmUtils/execute_EVM_commands.pl commands.list | tee run.log
     cat commands.list | parallel -j {threads} \"echo {{}} | bash\"
     #Combining the Partitions
     $EVM_HOME/EvmUtils/recombine_EVM_partial_outputs.pl --partitions partitions_list.out --output_file_name evm.out
     #convert to gff3
     $EVM_HOME/EvmUtils/convert_EVM_outputs_to_GFF3.pl  --partitions partitions_list.out --output evm.out --genome {WORKDIR}/{input.genome}
     cat `find . -name \*.out.gff3` > {WORKDIR}/{output}
     """


rule prepareGFFforEVM:
      input: augustus="./gff/"+ID+".augustus.gff", pasa="./reads/"+ID+".PASA" #DN
      output: evmIn="./gff/"+ID+".evmIn.gff", augustus="./gff/"+ID+".augustus.evm.gff", 
      shell:"""
      $EVM_HOME/EvmUtils/misc/augustus_to_GFF3.pl {input.augustus} > {output.augustus}
      cat {output.augustus} {input.pasa}/{ID}.pasa_assemblies.gff3 {DNGFF} | grep -vP "^#" > {output.evmIn}
      """


##################################
##                               #
##      AUGUSTUS                 #
##                               #
##################################

# Augustus trained with PASA

rule augustus:
     input: genome=ID+".fasta", hints="./gff/"+ID+".PASAhints.gff", pasa="./reads/"+ID+".PASAtrainingset"
     output: gff="./gff/"+ID+".augustus.gff"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:""" 
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     #create new species
     new_species.pl --species={ID}
     #copy extrinsic file
     cp -r $AUGUSTUS_CONFIG_PATH/extrinsic/extrinsic.E.cfg extrinsic.{ID}.cfg
     #start training
     gff2gbSmallDNA.pl {WORKDIR}/{input.pasa}/{ID}.assemblies.fasta.transdecoder.genome.gff3 {WORKDIR}/{input.genome} 10 genes.gb
     #set number
     randomSplit.pl genes.gb 200
     etraining --species={ID} genes.gb.train
     optimize_augustus.pl --species={ID} --cpus={threads} genes.gb.train
     etraining --species={ID} genes.gb.train
     augustus --gff3=on --species={ID} --hintsfile={WORKDIR}/{input.hints} --extrinsicCfgFile=extrinsic.{ID}.cfg {WORKDIR}/{input.genome} > {WORKDIR}/{output.gff}
     """

rule PASAhints:
     input: "./reads/"+ID+".PASA"
     output: "./gff/"+ID+".PASAhints.gff"
     params: cluster="-cwd -V"
     threads: THREADS
     shell:"""  
     perl {WORKDIR}/introncalc2.0.pl < {WORKDIR}/{input}/{ID}.pasa_assemblies.gff3 > {WORKDIR}/{output}
"""



##################################
##                               #
##      PASA  FIRST PASS         #
##                               #
##################################

rule PASA:
     input: genome=ID+".fasta", dn="./reads/"+ID+".trinityDN", gg="./reads/"+ID+".trinityGG", cf="./gff/"+ID+".cufflinks.gtf"
     output: dir="./reads/"+ID+".PASA" 
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     #mysql start
     cd $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/scripts/mysql_install_db --no-defaults --datadir=$HOME/bin/mysql-5.6.22/data
     cp $HOME/my-new.cnf $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/bin/mysqld_safe --defaults-file=./my-new.cnf --skip-grant-tables &
     sleep 10
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/PASA${{prefix}}
     cd {COMPUTEDIR}/PASA${{prefix}}       
     #Prepare files
     cat {WORKDIR}/{input.dn}/Trinity.fasta {WORKDIR}/{input.gg}/Trinity-GG.fasta > {COMPUTEDIR}/PASA${{prefix}}/transcripts.fasta
     # NOT genome-guided, only de novo
     $PASAHOME/misc_utilities/accession_extractor.pl < {WORKDIR}/{input.dn}/Trinity.fasta > {COMPUTEDIR}/PASA${{prefix}}/tdn.accs
     #Prepare alignAssembly.config
     DBNAME={ID}
     cat $PASAHOME/pasa_conf/pasa.alignAssembly.Template.txt | sed -r "s/DBNAME/${{DBNAME}}/" > ./alignAssembly.config
     $PASAHOME/scripts/Launch_PASA_pipeline.pl -c ./alignAssembly.config -C -r -R -g {WORKDIR}/{input.genome} --ALIGNERS blat,gmap -t ./transcripts.fasta --transcribed_is_aligned_orient --TDN ./tdn.accs --cufflinks_gtf {WORKDIR}/{input.cf} -I {INTRON} --stringent_alignment_overlap 30.0 --CPU {threads} 
     $PASAHOME/scripts/build_comprehensive_transcriptome.dbi -c ./alignAssembly.config -t ./transcripts.fasta --min_per_ID 95 --min_per_aligned 30
     mv {COMPUTEDIR}/PASA${{prefix}} {WORKDIR}/{output.dir}     
     #shutdown mysql
     cd $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/bin/mysqladmin --defaults-file=./my-new.cnf shutdown 
     """


rule PASAtrainingset:
     input: "./reads/"+ID+".PASA"
     output: dir="./reads/"+ID+".PASAtrainingset"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""        
     cd {WORKDIR}/{input}
     $PASAHOME/scripts/pasa_asmbls_to_training_set.dbi --pasa_transcripts_fasta {WORKDIR}/{input}/{ID}.assemblies.fasta --pasa_transcripts_gff3 {WORKDIR}/{input}/{ID}.pasa_assemblies.gff3
     mkdir {WORKDIR}/{output}
     mv {ID}.assemblies.fasta.transdecoder.* {WORKDIR}/{output}
     """

##################################
##                               #
##      TRINITY                  #
##                               #
##################################
#Can we really rely on trinity to make the assemblies ?
#Apparently it works at least as good as newbler and MIRA for 454 data, which are similar to ion torrent
#http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0051188

rule mergeFastq:
     input: expand("./reads/{samples}.fastq", samples=BAMS)
     output: "./reads/"+ID+".merged.fq"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     cat {WORKDIR}/{input} > {WORKDIR}/{output} 
     """

rule normalization:
     input: "./reads/"+ID+".merged.fq"
     output: "./reads/"+ID+".trinityIn"
     params: cluster="--partition=mem_0128 --qos=normal_0128"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinityIn${{prefix}}
     $HOME/bin/trinityrnaseq-2.0.6/util/insilico_read_normalization.pl --seqType fq --single {input} --SS_lib_type F --JM 110G --max_cov 30 --CPU {threads} --output {COMPUTEDIR}/trinityIn${{prefix}}  
     mv {COMPUTEDIR}/trinityIn${{prefix}} {WORKDIR}/{output}
     """
    
rule trinityDeNovo:
     input: "./reads/"+ID+".trinityIn" #"./reads/"+ID+".merged.fastq"
     output: "./reads/"+ID+".trinityDN"
     params: cluster="--partition=mem_0128 --qos=normal_0128"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinity${{prefix}}
     fastQName=`ls {WORKDIR}/{input}/*.ok  | sed -r 's/.ok//'`
     Trinity --seqType fq --single ${{fastQName}}  --SS_lib_type F --CPU {threads} --output {COMPUTEDIR}/trinity${{prefix}}  --max_memory 100G
     mv {COMPUTEDIR}/trinity${{prefix}} {WORKDIR}/{output}
     """


rule trinityAlignment:
     input: "./reads/"+ID+".merged.bam"
     output: dire="./reads/"+ID+".trinityGG"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinity${{prefix}}
     Trinity --genome_guided_bam {input} --genome_guided_max_intron {INTRON} --SS_lib_type F --max_memory 60G --CPU {threads} --output {COMPUTEDIR}/trinity${{prefix}}
     mv {COMPUTEDIR}/trinity${{prefix}} {WORKDIR}/{output}
     """


##################################
##                               #
##      CUFFLINKS                #
##                               #
##################################


rule mergeAssemblies:
     input: "./reads/assemblies.txt"
     output: dir="./reads", file="./gff/"+ID+".cufflinks.gtf"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     cuffmerge -o {WORKDIR}/{output.dir} -s {REF} {input} -p {threads}
     mv {WORKDIR}/{output.dir}/merged.gtf {output.file}
     """
        

rule composeMerge:
     input: expand("./reads/{samples}.cufflinks/transcripts.gtf", samples=BAMS)
     output: txt="./reads/assemblies.txt"
     shell:"""
     ls {WORKDIR}/{input} > {output.txt}
     """

rule cufflinks:
     input: "./reads/{samples}.mapped.sam"
     output: dir="./reads/{samples}.cufflinks", file="./reads/{samples}.cufflinks/transcripts.gtf"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cufflinks -o {COMPUTEDIR}/${{prefix}} -p {threads} -u -I {INTRON} --max-bundle-length 10000 --library-type ff-firststrand --min-intron-length 30 {WORKDIR}/{input}
     mv {COMPUTEDIR}/${{prefix}}/* {WORKDIR}/{output.dir}
     """


##################################
##                               #
##     Spliced RNAseq mapping    #
##                               #
##################################


rule mergeBam:
     input: expand("./reads/{samples}.mapped.bam", samples=BAMS)
     output: "./reads/"+ID+".merged.bam"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""       
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     samtools merge {COMPUTEDIR}/${{prefix}}/merged.bam {input}
     samtools sort {COMPUTEDIR}/${{prefix}}/merged.bam {COMPUTEDIR}/${{prefix}}/merged.sorted
     #mergeBam INPUT={WORKDIR}/{input} SORT_ORDER=coordinate OUTPUT={WORKDIR}/{output}
     mv {COMPUTEDIR}/${{prefix}}/merged.sorted.bam {WORKDIR}/{output}
     rm -rf {COMPUTEDIR}/${{prefix}}
     """


rule STAR:
     input: reads="./reads/{samples}.fastq", idx=ID+".idx"
     output: bam="./reads/{samples}.mapped.bam", sam="./reads/{samples}.mapped.sam"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     STAR --genomeDir {WORKDIR}/{input.idx} --readFilesIn {WORKDIR}/{input.reads} --runThreadN {threads}  --twopassMode Basic --outReadsUnmapped None --chimSegmentMin 12  --chimJunctionOverhangMin 12 --alignSJDBoverhangMin 10 --alignIntronMax {INTRON} --chimSegmentReadGapMax parameter 3  --alignSJstitchMismatchNmax 5 -1 5 5
     #sort sam files
     samtools view -bS {COMPUTEDIR}/${{prefix}}/Aligned.out.sam | samtools sort -  {COMPUTEDIR}/${{prefix}}/Aligned.sorted
     samtools view -h {COMPUTEDIR}/${{prefix}}/Aligned.sorted.bam | awk 'BEGIN {{OFS="\t"}} {{split($6,C,/[0-9]*/); split($6,L,/[SMDIN]/); if (C[2]=="S") {{$10=substr($10,L[1]+1); $11=substr($11,L[1]+1)}}; if (C[length(C)]=="S") {{L1=length($10)-L[length(L)-1]; $10=substr($10,1,L1); $11=substr($11,1,L1); }}; gsub(/[0-9]*S/,"",$6); print}}' > {COMPUTEDIR}/${{prefix}}/Aligned.sorted.sam
     #mv results to final directory
     mv {COMPUTEDIR}/${{prefix}}/Aligned.sorted.sam {WORKDIR}/{output.sam}
     mv {COMPUTEDIR}/${{prefix}}/Aligned.sorted.bam {WORKDIR}/{output.bam}
     #clean up
     rm -rf {COMPUTEDIR}/${{prefix}}
     """


rule STARIdx:
     input: genome=ID+".fasta"
     output: dir=ID+".idx"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     STAR --sjdbGTFfile {DNGTF} -sjdbOverhang 100 --runMode genomeGenerate --genomeDir {COMPUTEDIR}/${{prefix}} --genomeFastaFiles {WORKDIR}/{input} --runThreadN {threads} 
     mv {COMPUTEDIR}/${{prefix}} {WORKDIR}/{output.dir}
     """

#rule bamToFastq:
#     input: expand("./reads/{samples}.bam", samples=BAMS)
#     output: expand("./reads/{samples}.fastq", samples=BAMS)
#     params: cluster="--partition=mem_0064 --qos=normal_0064"
#     threads: THREADS
#     shell:"""
#     print {input}
#     parallel --no-notice -j {threads}  'bamToFastq -i {{}} -fq {{.}}.fastq' ::: {input}
#     """

rule clean:
     shell: "rm -rf *.sizes *.masked *.split *.lastz *.psl *.chain *.preChain *.net *.maf *.axt *.sh.* *.out *.tbl *.cat *.scipio"

##Masking
#rule repeatMasker:
 #    input: ID+".fasta"
  #   output: ID+".fasta.masked"
   #  threads: 16
    # params: cluster="-cwd -V"     
     #shell:"""
      #      RepeatMasker -qq -pa {threads} -species fungi {WORKDIR}/{input}
       #    """

