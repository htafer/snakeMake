"""
RNA-Seq Based Genome Annotation Pipeline
---------------------------------------
This Snakemake workflow performs comprehensive genome annotation using RNA-seq data:

1. RNA-seq Processing:
   - Read alignment (STAR)
   - Transcript assembly (Trinity, Cufflinks)
   - PASA alignment and assembly

2. Annotation:
   - Protein-coding gene identification
   - lncRNA prediction (CPAT)
   - Novel transcript discovery
   - Functional annotation (BLAST, PFAM)

Usage:
    snakemake -d `pwd` -s `pwd`/genomeAnnotationRNASeq.Snakemake \
        --stats snakemake.stats -j 100 --cluster 'sbatch {params.cluster}'

Requirements:
- STAR
- Trinity
- Cufflinks
- PASA
- CPAT
- BLAST+
- PFAM
- BEDTools
- MySQL (for PASA)

Author: htafer
Last Updated: 2025-07-28
"""

import math
import os
import fnmatch
from typing import List, Optional, Callable, Set, Any
from snakemake.utils import min_version

# Ensure minimum Snakemake version
min_version("6.0.0")

# Utility Functions
def count_fasta_entries(file_name: str) -> int:
    """
    Count the number of FASTA entries in a file.
    
    Args:
        file_name: Path to the FASTA file
        
    Returns:
        Total number of sequences (entries starting with '>')
    """
    with open(file_name) as f:
        return sum(1 for line in f if line.startswith(">"))

def calculate_chunks(total_entries: int, entries_per_chunk: int) -> int:
    """
    Calculate number of chunks needed for parallel processing.
    
    Args:
        total_entries: Total number of entries to process
        entries_per_chunk: Maximum entries per chunk
        
    Returns:
        Number of chunks needed
    """
    return math.floor(2 * total_entries / entries_per_chunk + 1)

def get_bam_files(dir_path: str, species: str) -> List[str]:
    """
    Get list of BAM files for a species.
    
    Args:
        dir_path: Base directory path
        species: Species identifier
        
    Returns:
        List of BAM file paths that match the pattern
    """
    species_dir = os.path.join(dir_path, f"{species}.reads")
    return [f for f in os.listdir(species_dir) 
            if fnmatch.fnmatch(f, "*.*.bam")]

def get_unique_elements(seq: List[Any], 
                       key_func: Optional[Callable[[Any], Any]] = None) -> List[Any]:
    """
    Get unique elements from a sequence while preserving order.
    
    Args:
        seq: Input sequence
        key_func: Optional function to extract comparison key
        
    Returns:
        List of unique elements
    """
    seen: Set[Any] = set()
    return [x for x in seq 
            if not (key_func(x) if key_func else x) in seen 
            and not seen.add(key_func(x) if key_func else x)]



# Configuration
#-------------

# Environment Setup
WORKDIR = "/home/lv70539/htafer/genomeAnnotationTranscript"  # Working directory
COMPUTEDIR = "/global/lv70539/htafer"  # Temporary computation directory

# Computational Resources
THREADS = 16  # Number of parallel threads

# Input Files
# -----------

# Genome
ID = "claImm"  # Genome identifier
REF = f"{ID}.fasta"  # Reference genome FASTA

# RNA-seq Data
BAMFILES = "./reads/{samples}.fastq"  # RNA-seq FASTQ pattern
BAMS, = glob_wildcards(BAMFILES)  # Available RNA-seq samples

# Reference Files
DNGFF = f"{WORKDIR}/{ID}.allEvm2.gff"  # De novo gene predictions
DNGTF = f"{WORKDIR}/{ID}.allEvm2.gtf"  # De novo gene predictions (GTF)

# Databases
PFAM = "/global/lv70539/htafer/share/"  # PFAM database directory
BLAST = "/global/lv70539/htafer/share/eurotiomycetesUniref90.clean.fasta"  # BLAST database

# EVM Configuration
EVMWEIGHT = f"{WORKDIR}/weightFile.cfg"  # EVM weights configuration

# Biological Parameters
INTRON = 2000  # Maximum intron length (bp)

# Execution Control
# ----------------

# Rules to run locally (not on cluster)
localrules: (
    # Core rules
    all, 
    clean,
    # Assembly rules
    composeMerge,
    mergeAssemblies,
    # PASA rules
    PASAtrainingset,
    PASAhints,
    # Annotation rules
    prepareGFFforEVM,
    # File handling
    mergeFastq,
    STARIdx,
    # Transcript analysis
    otherTranscript,
    lncRNA
)

# Rules to run on cluster (commented for reference)
# cluster_rules = [
#     'STAR',         # Read alignment
#     'mergeBam',     # BAM processing
#     'cufflinks',    # Transcript assembly
#     'trinityDeNovo',  # De novo assembly
#     'trinityAlignment',  # Genome-guided assembly
#     'normalization',  # Read normalization
#     'PASA',          # Transcript processing
#     'augustus',      # Gene prediction
#     'EVM',           # Evidence modeler
#     'postPASA'       # Post-processing
# ]

# Directory Structure
DIRS = {
    'logs': 'logs',
    'reads': 'reads',
    'gff': 'gff',
    'blast': 'BLAST',
    'pfam': 'PFAM',
    'cpat': 'CPAT'
}

# Create necessary directories
for dir_path in DIRS.values():
    os.makedirs(dir_path, exist_ok=True)

##################################
##                               #
##      ALL                      #
##                               #
##################################


# Default Target Rule
rule all:
    """
    Generate complete annotation including coding genes,
    lncRNAs, and other transcripts.
    """
    input: 
        transcripts = f"{ID}.noKnownFunc.gff3"

# Transcript Classification Rules
#-----------------------------

# Identify Novel Transcripts
rule otherTranscript:
    """
    Identify transcripts that are neither protein-coding
    nor long non-coding RNAs (potential novel RNA types).
    """
    input:
        lnc = f"{ID}.lncRNA.gff3",
        coding = f"gff/{ID}.allProtein.gff3",
        pasa = f"reads/{ID}.PASA/{ID}.pasa_assemblies.gff3"
    output:
        novel = f"{ID}.noKnownFunc.gff3"
    log:
        "logs/transcripts/other_transcripts.log"
    shell:
        """
        # Merge aligned transcripts
        mergealign.pl < {input.pasa} | \
            cut -f 1-9 > merged.gff 2> {log}

        # Find transcripts not overlapping with known types
        bedtools intersect \
            -s -v \
            -a merged.gff \
            -b {input.lnc} | \
        bedtools intersect \
            -s -v \
            -a - \
            -b {input.coding} | \
        cut -f 9 | \
        cut -f 1 -d " " | \
        sort -u | \
        fgrep -wf - {input.pasa} \
        > {output.novel} 2>> {log}
        """

# Long Non-coding RNA Identification
rule lncRNA:
    """
    Identify long non-coding RNAs using CPAT predictions
    and excluding transcripts with protein-coding evidence
    from BLAST and PFAM.
    """
    input:
        cpat = "CPAT/cpat.results",
        blast = "BLAST/blast.coding.candidates",
        pfam = f"PFAM/{ID}.pfam.coding.candidates",
        pasa = f"reads/{ID}.PASA/{ID}.pasa_assemblies.gff3"
    output:
        lnc = f"{ID}.lncRNA.gff3"
    threads: THREADS
    log:
        "logs/transcripts/lncrna.log"
    shell:
        """
        # Combine protein-coding evidence
        cat {WORKDIR}/{input.blast} {WORKDIR}/{input.pfam} | \
            sort -u > coding.candidates 2> {log}

        # Extract CPAT IDs
        cut -f 1 {WORKDIR}/{input.cpat} > id.cpat.results

        # Find transcripts without protein-coding evidence
        cat id.cpat.results | \
        perl -lane '
            my $count=`grep -cw $F[0] coding.candidates`;
            chomp($count);
            if($count==0){{print $F[0]}};
        ' | \
        sort -u | \
        fgrep -w -f - {WORKDIR}/{input.pasa} \
        > {WORKDIR}/{output.lnc} 2>> {log}
        """
# Protein Evidence Analysis Rules
#-----------------------------

# BLAST Search Against Known Proteins
rule blast:
    """
    Search transcript sequences against protein database using BLASTX
    to identify potential protein-coding transcripts.
    
    Parameters:
        E-value cutoff: 0.001
        Output format: tabular (format 7)
    """
    input:
        transcripts = f"CPAT/{ID}.cpat.fasta"
    output:
        candidates = "BLAST/blast.coding.candidates"
    threads: THREADS
    params:
        cluster = "--partition=mem_0064 --qos=normal_0064",
        evalue = 0.001,
        outfmt = 7  # Tabular format with comments
    log:
        "logs/blast/search.log"
    shell:
        """
        # Create output directory
        mkdir -p BLAST
        cd BLAST

        # Run BLASTX search
        blastx \
            -query {WORKDIR}/{input.transcripts} \
            -db {BLAST} \
            -num_threads {threads} \
            -outfmt {params.outfmt} \
            -evalue {params.evalue} \
            -out blast.out \
            2> {WORKDIR}/{log}

        # Filter results and format output
        cat blast.out | \
        perl -lane '
            # Check score and E-value
            if($F[7]>$F[6] && $F[10]<{params.evalue}) {{
                print;
            }}
        ' | \
        cut -f 1 | \
        sort -u | \
        sed -r 's/asmbl/>asmbl/' \
        > {WORKDIR}/{output.candidates}
        """

# PFAM Domain Search
rule pfam:
    """
    Search for protein domains in transcript sequences using PFAM
    to identify potential protein-coding transcripts.
    
    Parameters:
        E-value cutoff: 0.001
        Multi-threading enabled
    """
    input:
        transcripts = f"CPAT/{ID}.cpat.fasta"
    output:
        candidates = f"PFAM/{ID}.pfam.coding.candidates"
    threads: THREADS
    params:
        cluster = "--partition=mem_0064 --qos=normal_0064",
        evalue = 0.001
    log:
        "logs/pfam/search.log"
    shell:
        """
        # Create output directory
        mkdir -p PFAM
        cd PFAM

        # Run PFAM search
        pfam_scan.pl \
            -fasta {WORKDIR}/{input.transcripts} \
            -dir {PFAM} \
            -cpu {threads} \
            -outfile pfam.out \
            2> {WORKDIR}/{log}

        # Filter results and format output
        cat pfam.out | \
        grep -v '#' | \
        perl -lane '
            # Check domain coordinates and E-value
            if($F[2]>$F[1] && $F[12]<{params.evalue}) {{
                print;
            }}
        ' | \
        cut -f 1 | \
        sort -u | \
        sed -r 's/asmbl/>asmbl/' \
        > {WORKDIR}/{output.candidates}
        """



# CPAT Analysis Rules
#------------------

# Run Coding Potential Assessment
rule cpat:
    """
    Run Coding Potential Assessment Tool (CPAT) to identify
    potential coding and non-coding transcripts.
    
    Parameters:
        Coding probability cutoff: 0.01
        Uses hexamer frequency and logistic regression model
    """
    input:
        gene = "noOverlap.fasta",          # Test sequences
        assemblies = "assemblies.fa",       # All assemblies
        noncoding = "nonconding.fasta",    # Known non-coding sequences
        coding = "codingseq.fasta"         # Known coding sequences
    output:
        fasta = f"CPAT/{ID}.cpat.fasta",   # Filtered sequences
        results = "CPAT/cpat.results"       # CPAT predictions
    threads: THREADS
    params:
        prob_cutoff = 0.01,    # Coding probability cutoff
        python_ver = "2.7"     # Required Python version
    log:
        "logs/cpat/analysis.log"
    shell:
        """
        # Create output directory
        mkdir -p CPAT
        cd CPAT

        # Generate hexamer frequency table
        make_hexamer_tab.py \
            -c {WORKDIR}/{input.coding} \
            -n {WORKDIR}/{input.noncoding} \
            > hexamer.tab 2> {WORKDIR}/{log}

        # Create logistic regression model
        make_logitModel.py \
            -c {WORKDIR}/{input.coding} \
            -n {WORKDIR}/{input.noncoding} \
            -x hexamer.tab \
            -o {ID} 2>> {WORKDIR}/{log}

        # Load required Python version
        module load python/{params.python_ver}

        # Run CPAT
        cpat.py \
            -d {ID}.logit.RData \
            -x hexamer.tab \
            -g {WORKDIR}/{input.gene} \
            -o {ID}.cpat 2>> {WORKDIR}/{log}

        # Filter and format results
        cat {ID}.cpat | \
        perl -lane '
            if($F[5]<{params.prob_cutoff}) {{
                print;
            }}
        ' | \
        sort -k 2,2gr | \
        sed -r 's/ASMBL/asmbl/' \
        > {WORKDIR}/{output.results}

        # Extract sequences for filtered results
        parallel -j {threads} \
            'grep -P "{{}}$" {WORKDIR}/{input.assemblies} -A 1' \
            ::: $(cat {WORKDIR}/{output.results} | cut -f 1 | grep asmbl) \
            > {WORKDIR}/{output.fasta}
        """

# Prepare Training Files for CPAT
rule prepareFiles:
    """
    Prepare training datasets for CPAT by extracting known
    coding and non-coding sequences.
    """
    input:
        assemblies = "assemblies.fa",
        introns = f"gff/{ID}.PASAhints.gff",
        annotation = f"gff/{ID}.allProtein.gff3"
    output:
        coding = "codingseq.fasta",      # Known coding sequences
        noncoding = "nonconding.fasta"   # Known non-coding sequences
    threads: THREADS
    log:
        "logs/cpat/prepare_files.log"
    shell:
        """
        # Extract coding sequences from annotation
        getAnnoFasta.pl {input.annotation} \
            --seqfile {REF} 2> {log}

        # Process coding sequences
        cat "./gff/{ID}.allProtein3.cdsexons" | \
        sed -r 's/.cds[0-9]//g' | \
        perl -lane '
            BEGIN{{my $previousId="";}}
            if($F[0]=~/>/) {{
                if($F[0] eq $previousId) {{
                    next;
                }} else {{
                    $previousId=$F[0];
                    print "\\n",$F[0]
                }}
            }} else {{
                printf $F[0]
            }}
        ' > {output.coding} 2>> {log}

        # Process non-coding (intron) sequences
        cat {input.introns} | \
            sed -r 's/;src=E//g' > temp.introns

        parallel -j {threads} \
            'grep -P "{{}}$" {input.assemblies} -A 1' \
            ::: $(cat temp.introns | \
                  cut -f 9 | \
                  sed -r 's/.+Target=([^$]+)/\\1/' | \
                  sort -u) \
            > {output.noncoding} 2>> {log}

        # Cleanup
        rm -f temp.introns
        """

################
#   INTERSECT  #
################
rule intersect:
     input: transAnn="./reads/"+ID+".PASAtrainingset/"+ID+".assemblies.fasta.transdecoder.genome.gff3", pasaAssemblies="./reads/"+ID+".PASA/"+ID+".pasa_assemblies.gff3", pasaSequence="./reads/"+ID+".PASA/"+ID+".assemblies.fasta", updateAnn="./gff/"+ID+".allProtein.gff3"
     output: fasta="noOverlap.fasta", fa="assemblies.fa"
     params:
     threads: THREADS
     shell:"""                      
     #put the gene together         
     mergealign.pl < {input.pasaAssemblies} | cut -f 1-9 >  merged.gff      
     #check which gene overlap neither with the updated protein annotation nor with the pasa protein annotation     
     intersectBed -v -s -a merged.gff -b {input.updateAnn} > no.gff3        
     intersectBed -v -s -a no.gff3 -b {input.transAnn} > noOverlap.gff      
     perl ~/bin/rmenterdb.pl < {input.pasaSequence} > {output.fa}           
     #Fetch the sequences           
     parallel -j {threads} 'grep -P "{{}}$" {output.fa} -A 1 ' ::: `cat noOverlap.gff | cut -f 9 | sed -r 's/.+Target=([^$]+)/\\1/' | sort -u` > {output.fasta} 
"""




#################################################################################
#                                                                               #
#                  Get New Proteins                                             #
#                                                                               #
#################################################################################                  

rule newProtein:
       input: transAnn="./reads/"+ID+".PASAtrainingset/"+ID+".assemblies.fasta.transdecoder.genome.gff3", updateAnn="./gff/"+ID+".Annotation/"+ID+".gff3"
       output: newProtein="./gff/"+ID+".newProtein.gff3", allProtein="./gff/"+ID+".allProtein.gff3"
       shell:"""       
       grep -P "\tCDS\t" {input.transAnn} | mergealign.pl > {ID}.transdecoder.CDS.gff3                 
       grep -P "\tCDS\t" {input.updateAnn} | mergealign.pl > {ID}.update.CDS.gff3                      
       bedtools intersect -v -a {ID}.transdecoder.CDS.gff3 -b {ID}.update.CDS.gff3 | cut -f 9 | sed -r 's/.+Parent=//' | fgrep -f - {input.transAnn} | grep -P "\tCDS\t" > {ID}.newProtein.CDS.gff3                            
       cut -f 9 {ID}.newProtein.CDS.gff3 | sed -r 's/.+Parent=//' | fgrep -f - {input.transAnn} | perl -lane 'if($F[2]=~/mRNA/){{$anno=$F[8]; $anno=~s/ID=.+Parent=/ID=/; $anno=~/=(asmbl[^g]+g[^;]+)/; my  $id=$1; $id=~s/g\./m./; $anno=~s/ORF/$id/; print "$F[0]\t$F[1]\tgene\t$F[3]\t$F[4]\t$F[5]\t$F[6]\t$F[7]\t$anno"; $anno=$F[8]; $anno=~s/ORF/$id/; print join("\t",@F[0..7]),"\t$ann\
o";}}else{{print;}}' > {output.newProtein}                     
       cat {input.updateAnn} {output.newProtein}  > {output.allProtein}                                
       """



#################################################################################
#                                                                               #
#                           PASA 2nd run                                        #
#                                                                               #
#################################################################################

rule postPASA:
      input: genome=ID+".fasta", pred="./gff/"+ID+".all.evm.gff3", dn="./reads/"+ID+".trinityDN", gg="./reads/"+ID+".trinityGG"
      output: dir="./gff/"+ID+".Annotation", updateGFF3=ID+".Annotation/"+ID+".gff3"
      params: cluster="--partition=mem_0064 --qos=normal_0064"
      threads: THREADS
      shell: """
      #mysql start
      cd $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/scripts/mysql_install_db --no-defaults --datadir=$HOME/bin/mysql-5.6.22/data
      cp $HOME/my-new.cnf $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/bin/mysqld_safe --defaults-file=./my-new.cnf --skip-grant-tables &
      sleep 10
      prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
      mkdir -p {COMPUTEDIR}/PASA${{prefix}}
      cd {COMPUTEDIR}/PASA${{prefix}}
      #check compatibility
      #$PASAHOME/misc_utilities/pasa_gff3_validator.pl {WORKDIR}/{input.pred}
      cat {WORKDIR}/{input.dn}/Trinity.fasta {WORKDIR}/{input.gg}/Trinity-GG.fasta > {COMPUTEDIR}/PASA${{prefix}}/transcripts.fasta
      #Prepare annotCompare.config
      DBNAME={ID}
      cat $PASAHOME/pasa_conf/pasa.annotationCompare.Template.txt | sed -r "s/DBNAME/${{DBNAME}}/" > ./annotCompare.config
      $PASAHOME/scripts/Launch_PASA_pipeline.pl -c ./annotCompare.config -A -L --annots_gff3 {WORKDIR}/{input.pred} -g {WORKDIR}/{input.genome} -t ./transcripts.fasta
      mv {COMPUTEDIR}/PASA${{prefix}} {WORKDIR}/{output.dir}
      cd {WORKDIR}/{output.dir} 
      mv *gene_structures_post_PASA_updates.*.gff3 {output.upateGFF3}
      #mysql shutdown
      cd $HOME/bin/mysql-5.6.22/
      $HOME/bin/mysql-5.6.22/bin/mysqladmin --defaults-file=./my-new.cnf shutdown
      """




#################################################################################   
#                                                                               #     
#                            EVM                                                #     
#                                                                               #     
#################################################################################  

rule EVM:
     input: evmIn="./gff/"+ID+".evmIn.gff", pasa="./reads/"+ID+".PASA", genome=ID+".fasta"
     output: "./gff/"+ID+".all.evm.gff3"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     #partitioning the inputs
     $EVM_HOME/EvmUtils/partition_EVM_inputs.pl --genome {WORKDIR}/{input.genome} --gene_predictions {WORKDIR}/{input.evmIn} --transcript_alignments {WORKDIR}/{input.pasa}/{ID}.pasa_assemblies.gff3 --segmentSize 1000000 --overlapSize 20000 --partition_listing partitions_list.out
     #generating the EVM Command Set
     $EVM_HOME/EvmUtils/write_EVM_commands.pl --genome {WORKDIR}/{input.genome} --weights {EVMWEIGHT} --gene_predictions {WORKDIR}/{input.evmIn} --transcript_alignments {WORKDIR}/{input.pasa}/{ID}.pasa_assemblies.gff3 --output_file evm.out --partitions partitions_list.out  --min_intron_length 10 > commands.list
     #$EVM_HOME/EvmUtils/execute_EVM_commands.pl commands.list | tee run.log
     cat commands.list | parallel -j {threads} \"echo {{}} | bash\"
     #Combining the Partitions
     $EVM_HOME/EvmUtils/recombine_EVM_partial_outputs.pl --partitions partitions_list.out --output_file_name evm.out
     #convert to gff3
     $EVM_HOME/EvmUtils/convert_EVM_outputs_to_GFF3.pl  --partitions partitions_list.out --output evm.out --genome {WORKDIR}/{input.genome}
     cat `find . -name \*.out.gff3` > {WORKDIR}/{output}
     """


rule prepareGFFforEVM:
      input: augustus="./gff/"+ID+".augustus.gff", pasa="./reads/"+ID+".PASA" #DN
      output: evmIn="./gff/"+ID+".evmIn.gff", augustus="./gff/"+ID+".augustus.evm.gff", 
      shell:"""
      $EVM_HOME/EvmUtils/misc/augustus_to_GFF3.pl {input.augustus} > {output.augustus}
      cat {output.augustus} {input.pasa}/{ID}.pasa_assemblies.gff3 {DNGFF} | grep -vP "^#" > {output.evmIn}
      """


##################################
##                               #
##      AUGUSTUS                 #
##                               #
##################################

# Augustus trained with PASA

rule augustus:
     input: genome=ID+".fasta", hints="./gff/"+ID+".PASAhints.gff", pasa="./reads/"+ID+".PASAtrainingset"
     output: gff="./gff/"+ID+".augustus.gff"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:""" 
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     #create new species
     new_species.pl --species={ID}
     #copy extrinsic file
     cp -r $AUGUSTUS_CONFIG_PATH/extrinsic/extrinsic.E.cfg extrinsic.{ID}.cfg
     #start training
     gff2gbSmallDNA.pl {WORKDIR}/{input.pasa}/{ID}.assemblies.fasta.transdecoder.genome.gff3 {WORKDIR}/{input.genome} 10 genes.gb
     #set number
     randomSplit.pl genes.gb 200
     etraining --species={ID} genes.gb.train
     optimize_augustus.pl --species={ID} --cpus={threads} genes.gb.train
     etraining --species={ID} genes.gb.train
     augustus --gff3=on --species={ID} --hintsfile={WORKDIR}/{input.hints} --extrinsicCfgFile=extrinsic.{ID}.cfg {WORKDIR}/{input.genome} > {WORKDIR}/{output.gff}
     """

rule PASAhints:
     input: "./reads/"+ID+".PASA"
     output: "./gff/"+ID+".PASAhints.gff"
     params: cluster="-cwd -V"
     threads: THREADS
     shell:"""  
     perl {WORKDIR}/introncalc2.0.pl < {WORKDIR}/{input}/{ID}.pasa_assemblies.gff3 > {WORKDIR}/{output}
"""



##################################
##                               #
##      PASA  FIRST PASS         #
##                               #
##################################

rule PASA:
     input: genome=ID+".fasta", dn="./reads/"+ID+".trinityDN", gg="./reads/"+ID+".trinityGG", cf="./gff/"+ID+".cufflinks.gtf"
     output: dir="./reads/"+ID+".PASA" 
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     #mysql start
     cd $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/scripts/mysql_install_db --no-defaults --datadir=$HOME/bin/mysql-5.6.22/data
     cp $HOME/my-new.cnf $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/bin/mysqld_safe --defaults-file=./my-new.cnf --skip-grant-tables &
     sleep 10
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/PASA${{prefix}}
     cd {COMPUTEDIR}/PASA${{prefix}}       
     #Prepare files
     cat {WORKDIR}/{input.dn}/Trinity.fasta {WORKDIR}/{input.gg}/Trinity-GG.fasta > {COMPUTEDIR}/PASA${{prefix}}/transcripts.fasta
     # NOT genome-guided, only de novo
     $PASAHOME/misc_utilities/accession_extractor.pl < {WORKDIR}/{input.dn}/Trinity.fasta > {COMPUTEDIR}/PASA${{prefix}}/tdn.accs
     #Prepare alignAssembly.config
     DBNAME={ID}
     cat $PASAHOME/pasa_conf/pasa.alignAssembly.Template.txt | sed -r "s/DBNAME/${{DBNAME}}/" > ./alignAssembly.config
     $PASAHOME/scripts/Launch_PASA_pipeline.pl -c ./alignAssembly.config -C -r -R -g {WORKDIR}/{input.genome} --ALIGNERS blat,gmap -t ./transcripts.fasta --transcribed_is_aligned_orient --TDN ./tdn.accs --cufflinks_gtf {WORKDIR}/{input.cf} -I {INTRON} --stringent_alignment_overlap 30.0 --CPU {threads} 
     $PASAHOME/scripts/build_comprehensive_transcriptome.dbi -c ./alignAssembly.config -t ./transcripts.fasta --min_per_ID 95 --min_per_aligned 30
     mv {COMPUTEDIR}/PASA${{prefix}} {WORKDIR}/{output.dir}     
     #shutdown mysql
     cd $HOME/bin/mysql-5.6.22/
     $HOME/bin/mysql-5.6.22/bin/mysqladmin --defaults-file=./my-new.cnf shutdown 
     """


rule PASAtrainingset:
     input: "./reads/"+ID+".PASA"
     output: dir="./reads/"+ID+".PASAtrainingset"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""        
     cd {WORKDIR}/{input}
     $PASAHOME/scripts/pasa_asmbls_to_training_set.dbi --pasa_transcripts_fasta {WORKDIR}/{input}/{ID}.assemblies.fasta --pasa_transcripts_gff3 {WORKDIR}/{input}/{ID}.pasa_assemblies.gff3
     mkdir {WORKDIR}/{output}
     mv {ID}.assemblies.fasta.transdecoder.* {WORKDIR}/{output}
     """

##################################
##                               #
##      TRINITY                  #
##                               #
##################################
#Can we really rely on trinity to make the assemblies ?
#Apparently it works at least as good as newbler and MIRA for 454 data, which are similar to ion torrent
#http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0051188

rule mergeFastq:
     input: expand("./reads/{samples}.fastq", samples=BAMS)
     output: "./reads/"+ID+".merged.fq"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     cat {WORKDIR}/{input} > {WORKDIR}/{output} 
     """

rule normalization:
     input: "./reads/"+ID+".merged.fq"
     output: "./reads/"+ID+".trinityIn"
     params: cluster="--partition=mem_0128 --qos=normal_0128"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinityIn${{prefix}}
     $HOME/bin/trinityrnaseq-2.0.6/util/insilico_read_normalization.pl --seqType fq --single {input} --SS_lib_type F --JM 110G --max_cov 30 --CPU {threads} --output {COMPUTEDIR}/trinityIn${{prefix}}  
     mv {COMPUTEDIR}/trinityIn${{prefix}} {WORKDIR}/{output}
     """
    
rule trinityDeNovo:
     input: "./reads/"+ID+".trinityIn" #"./reads/"+ID+".merged.fastq"
     output: "./reads/"+ID+".trinityDN"
     params: cluster="--partition=mem_0128 --qos=normal_0128"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinity${{prefix}}
     fastQName=`ls {WORKDIR}/{input}/*.ok  | sed -r 's/.ok//'`
     Trinity --seqType fq --single ${{fastQName}}  --SS_lib_type F --CPU {threads} --output {COMPUTEDIR}/trinity${{prefix}}  --max_memory 100G
     mv {COMPUTEDIR}/trinity${{prefix}} {WORKDIR}/{output}
     """


rule trinityAlignment:
     input: "./reads/"+ID+".merged.bam"
     output: dire="./reads/"+ID+".trinityGG"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/trinity${{prefix}}
     Trinity --genome_guided_bam {input} --genome_guided_max_intron {INTRON} --SS_lib_type F --max_memory 60G --CPU {threads} --output {COMPUTEDIR}/trinity${{prefix}}
     mv {COMPUTEDIR}/trinity${{prefix}} {WORKDIR}/{output}
     """


##################################
##                               #
##      CUFFLINKS                #
##                               #
##################################


rule mergeAssemblies:
     input: "./reads/assemblies.txt"
     output: dir="./reads", file="./gff/"+ID+".cufflinks.gtf"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""
     cuffmerge -o {WORKDIR}/{output.dir} -s {REF} {input} -p {threads}
     mv {WORKDIR}/{output.dir}/merged.gtf {output.file}
     """
        

rule composeMerge:
     input: expand("./reads/{samples}.cufflinks/transcripts.gtf", samples=BAMS)
     output: txt="./reads/assemblies.txt"
     shell:"""
     ls {WORKDIR}/{input} > {output.txt}
     """

rule cufflinks:
     input: "./reads/{samples}.mapped.sam"
     output: dir="./reads/{samples}.cufflinks", file="./reads/{samples}.cufflinks/transcripts.gtf"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cufflinks -o {COMPUTEDIR}/${{prefix}} -p {threads} -u -I {INTRON} --max-bundle-length 10000 --library-type ff-firststrand --min-intron-length 30 {WORKDIR}/{input}
     mv {COMPUTEDIR}/${{prefix}}/* {WORKDIR}/{output.dir}
     """


##################################
##                               #
##     Spliced RNAseq mapping    #
##                               #
##################################


rule mergeBam:
     input: expand("./reads/{samples}.mapped.bam", samples=BAMS)
     output: "./reads/"+ID+".merged.bam"
     params: cluster="--partition=mem_0064 --qos=normal_0064"
     threads: THREADS
     shell:"""       
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     samtools merge {COMPUTEDIR}/${{prefix}}/merged.bam {input}
     samtools sort {COMPUTEDIR}/${{prefix}}/merged.bam {COMPUTEDIR}/${{prefix}}/merged.sorted
     #mergeBam INPUT={WORKDIR}/{input} SORT_ORDER=coordinate OUTPUT={WORKDIR}/{output}
     mv {COMPUTEDIR}/${{prefix}}/merged.sorted.bam {WORKDIR}/{output}
     rm -rf {COMPUTEDIR}/${{prefix}}
     """


rule STAR:
     input: reads="./reads/{samples}.fastq", idx=ID+".idx"
     output: bam="./reads/{samples}.mapped.bam", sam="./reads/{samples}.mapped.sam"
     params: cluster="--partition=mem_0064 --qos=normal_0064"     
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     cd {COMPUTEDIR}/${{prefix}}
     STAR --genomeDir {WORKDIR}/{input.idx} --readFilesIn {WORKDIR}/{input.reads} --runThreadN {threads}  --twopassMode Basic --outReadsUnmapped None --chimSegmentMin 12  --chimJunctionOverhangMin 12 --alignSJDBoverhangMin 10 --alignIntronMax {INTRON} --chimSegmentReadGapMax parameter 3  --alignSJstitchMismatchNmax 5 -1 5 5
     #sort sam files
     samtools view -bS {COMPUTEDIR}/${{prefix}}/Aligned.out.sam | samtools sort -  {COMPUTEDIR}/${{prefix}}/Aligned.sorted
     samtools view -h {COMPUTEDIR}/${{prefix}}/Aligned.sorted.bam | awk 'BEGIN {{OFS="\t"}} {{split($6,C,/[0-9]*/); split($6,L,/[SMDIN]/); if (C[2]=="S") {{$10=substr($10,L[1]+1); $11=substr($11,L[1]+1)}}; if (C[length(C)]=="S") {{L1=length($10)-L[length(L)-1]; $10=substr($10,1,L1); $11=substr($11,1,L1); }}; gsub(/[0-9]*S/,"",$6); print}}' > {COMPUTEDIR}/${{prefix}}/Aligned.sorted.sam
     #mv results to final directory
     mv {COMPUTEDIR}/${{prefix}}/Aligned.sorted.sam {WORKDIR}/{output.sam}
     mv {COMPUTEDIR}/${{prefix}}/Aligned.sorted.bam {WORKDIR}/{output.bam}
     #clean up
     rm -rf {COMPUTEDIR}/${{prefix}}
     """


rule STARIdx:
     input: genome=ID+".fasta"
     output: dir=ID+".idx"
     threads: THREADS
     shell:"""
     prefix=`date --rfc-3339=ns  | md5sum | head -c 16`
     mkdir -p {COMPUTEDIR}/${{prefix}}
     STAR --sjdbGTFfile {DNGTF} -sjdbOverhang 100 --runMode genomeGenerate --genomeDir {COMPUTEDIR}/${{prefix}} --genomeFastaFiles {WORKDIR}/{input} --runThreadN {threads} 
     mv {COMPUTEDIR}/${{prefix}} {WORKDIR}/{output.dir}
     """

#rule bamToFastq:
#     input: expand("./reads/{samples}.bam", samples=BAMS)
#     output: expand("./reads/{samples}.fastq", samples=BAMS)
#     params: cluster="--partition=mem_0064 --qos=normal_0064"
#     threads: THREADS
#     shell:"""
#     print {input}
#     parallel --no-notice -j {threads}  'bamToFastq -i {{}} -fq {{.}}.fastq' ::: {input}
#     """

rule clean:
     shell: "rm -rf *.sizes *.masked *.split *.lastz *.psl *.chain *.preChain *.net *.maf *.axt *.sh.* *.out *.tbl *.cat *.scipio"

##Masking
#rule repeatMasker:
 #    input: ID+".fasta"
  #   output: ID+".fasta.masked"
   #  threads: 16
    # params: cluster="-cwd -V"     
     #shell:"""
      #      RepeatMasker -qq -pa {threads} -species fungi {WORKDIR}/{input}
       #    """

